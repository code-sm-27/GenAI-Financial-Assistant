# services/genai-inference-service/llm_client.py
from google.cloud import aiplatform
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class VertexAILLMClient:
    def __init__(self, project_id, endpoint_id, location="us-central1"):
        self.project_id = project_id
        self.endpoint_id = endpoint_id
        self.location = location
        self.endpoint = None
        try:
            aiplatform.init(project=self.project_id, location=self.location)
            # Initialize the endpoint client
            self.endpoint = aiplatform.Endpoint(endpoint_name=f"projects/{self.project_id}/locations/{self.location}/endpoints/{self.endpoint_id}")
            logging.info(f"Vertex AI client initialized for endpoint: {self.endpoint_id} in {self.location}")
        except Exception as e:
            logging.error(f"Failed to initialize Vertex AI client: {e}")
            self.endpoint = None # Ensure endpoint is None if initialization fails

    def generate_advice(self, prompt: str) -> str:
        """
        Sends a prompt to the Vertex AI LLM endpoint and returns the generated text.
        """
        if not self.endpoint:
            logging.error("Vertex AI endpoint not initialized. Cannot generate advice.")
            return "Error: AI service not available."

        try:
            # The structure of instances might vary based on your deployed model.
            # Assuming a text-bison or similar model where content is the main input.
            response = self.endpoint.predict(instances=[{"content": prompt}])

            predictions = response.predictions
            if predictions and isinstance(predictions, list) and len(predictions) > 0:
                # Assuming the response format has 'text' key in the first prediction
                generated_text = predictions[0].get("text", "No response generated.")
                logging.info(f"Successfully generated advice (first 100 chars): {generated_text[:100]}...")
                return generated_text
            else:
                logging.warning("Vertex AI returned no predictions or unexpected format.")
                return "No response generated by AI."
        except Exception as e:
            logging.error(f"Error during Vertex AI prediction: {e}")
            return f"Error generating advice: {e}"

# Example usage (for testing LLM client directly - requires GCP setup)
if __name__ == '__main__':
    # You MUST replace these with your actual GCP Project ID and Vertex AI Endpoint ID
    # and ensure your environment is authenticated (e.g., `gcloud auth application-default login`)
    YOUR_PROJECT_ID = "your-gcp-project-id"
    YOUR_ENDPOINT_ID = "your-vertex-ai-endpoint-id"

    if YOUR_PROJECT_ID == "your-gcp-project-id" or YOUR_ENDPOINT_ID == "your-vertex-ai-endpoint-id":
        print("WARNING: Please update YOUR_PROJECT_ID and YOUR_ENDPOINT_ID in llm_client.py for testing.")
        print("Skipping direct LLM client test.")
    else:
        llm_client = VertexAILLMClient(YOUR_PROJECT_ID, YOUR_ENDPOINT_ID)

        test_prompt = "Explain why investing in SIPs is beneficial for long-term goals."
        print(f"\nSending test prompt to LLM: '{test_prompt}'")
        advice = llm_client.generate_advice(test_prompt)
        print("\nGenerated Advice:", advice)

        test_prompt_stock = "Based on a closing price of 1500 and high volume, what is a buy/sell/hold recommendation for Reliance Industries?"
        print(f"\nSending stock test prompt to LLM: '{test_prompt_stock}'")
        advice_stock = llm_client.generate_advice(test_prompt_stock)
        print("\nGenerated Advice for Stock:", advice_stock)